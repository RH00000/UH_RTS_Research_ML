{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNsHYCmAj/+ggB/+nZc3AKk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RH00000/UH_RTS_Research_ML/blob/main/non_skipping_idk_cascade_0.7_confidence.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision tensorflow-datasets"
      ],
      "metadata": {
        "id": "ivrc2earSZrh",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load the ImaageNetV2 TopImages split\n",
        "import tensorflow_datasets as tfds\n",
        "import torch\n",
        "from torch.utils.data import IterableDataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "# 1) Download & prepare the TFDS builder for ImageNetV2 TopImages\n",
        "builder = tfds.builder(\"imagenet_v2\", config=\"topimages\")\n",
        "builder.download_and_prepare()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 148,
          "referenced_widgets": [
            "5921914451cc4164aae0dc20c05dfa83",
            "f0b402bf39d142c786cd5d80fdce6d78",
            "2845f107a1554120882e2a1c43c2ba8a",
            "c9228d23b9b74b14b565f36c759f1295",
            "c333651168784308bbb29f12f6d31b3f",
            "db37ad9f7b234d0194e99737fc17deff",
            "939590d215d5434b92cbec348daa9c47",
            "549e6ff118d940299fb6a4954f5064c8",
            "035f8b444c824abaa936d6508dfc4de7",
            "a339974d912141659c25208d6321bfe5",
            "88254b340d344e34bcf46e7c2f629503",
            "b3985cf2d24d4eefaea6b973104e0c9e",
            "e9d3f8ba728142cdba667f2046e20504",
            "199fb265d5c948f5bb4c45b34d55308e",
            "aec19407664e424b86bbbe99fb868817",
            "bb448f38a172486193f1f119aecc1013",
            "071f458e3c73484885f9dcfb4dff00da",
            "b4eed05ce8924c52805654b8cfc5c6cf",
            "41d0aa32e51e4f92bff29283ad78f817",
            "8b0dad1628754d0080122899caed9a3d",
            "307792bbc2764c92890d51fd454a7f10",
            "d392b32709e74288b621924ccc064038",
            "ff72e83082304361b7229c66c21a66f9",
            "c8c57fbc257746b6a6f9ca6403e48324",
            "d3868743bdbb4c55883a19261f2d5ae6",
            "56f16cf191244177ad118b19af889cca",
            "c808186afb7048078601bcb3afc84ac9",
            "3623da8faa8c42e19e6bc804a2553146",
            "740f48fe92d447a3b85eda94e77b9f1e",
            "486407779b1d44e2920c95f5e715d4bf",
            "e02c813b4bc7416d8b4b9418bf1c0ea2",
            "829e3f75cc034716821d69de475e7d8c",
            "8b26947ec0c9499484f94c3bfb34c585",
            "987f669769974f2999f28993a027195f",
            "c94c3ac7a5224f8990539049c69d23c4",
            "e0f80fa9cad74fb5b5a9659a40bc3e7f",
            "2787ff806d824ccaa7dc8ad065dc020b",
            "138a201c382347aba9fd4a80af9b8876",
            "b0ea69b05e3d492b8c6e79e46055561a",
            "5aeb9eeeb6064713b5386c9bd8b15086",
            "417cd739cb0f44739dbb7736316fcd07",
            "4b663ef34a5b439b914405cbe2dc7477",
            "5e52f55a05284da9861d2d9b56c84f06",
            "aef4c1abbe2a44ac941bcbab1a8e9bc7",
            "d3d9818133a64d19b01c7a68396d283d",
            "3b783b5019c04d7c8c486ca3a4047f57",
            "aa38cb3b951b407c93cb5cd120630190",
            "e242bf51eab3495682feef597f43e721",
            "a318e9bbcbf541669323d0167af58c02",
            "264ecd663f45425a9c10987636e74c25",
            "48f58981cc30425ca27b67583cd72dd4",
            "83b91fdf61e048c194ca12b7ad74e290",
            "699ed0fa09894663b1ea33d4ebe31970",
            "dffda1b20abd411eb5bcef0dc9a99fa4",
            "c98ffa9a73b64d82a9adfb2cddcaf409",
            "720932a375d94345b0294e7ebcad83cb",
            "35301758c3ea41cc8daa3737af48df38",
            "2fea2fedd6ff41c5ba3b8409b71186d1",
            "732b8f6ea11c4774bc2f4a6dd307492d",
            "3dad20de4a08410799f99e776dad1274",
            "c8b729eaf5eb450fad5d25e28349f825",
            "0a6a742dadeb4b83a010e99dde67edbe",
            "b353f8ac3177471f96ffb4a108652e1b",
            "7c7dd5e012964fc4a8be6ade875d26ed",
            "77cca3151c7042da8ad583d60b20f652",
            "86da3326acd048bc8b8fd425d413ad5d"
          ]
        },
        "id": "GZ_6l-JgVxYU",
        "outputId": "e7a9fa97-5a3b-4f93-d23f-07368afe213f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to /root/tensorflow_datasets/imagenet_v2/topimages/3.0.0...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Dl Completed...: 0 url [00:00, ? url/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5921914451cc4164aae0dc20c05dfa83"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Dl Size...: 0 MiB [00:00, ? MiB/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b3985cf2d24d4eefaea6b973104e0c9e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Extraction completed...: 0 file [00:00, ? file/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ff72e83082304361b7229c66c21a66f9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating splits...:   0%|          | 0/1 [00:00<?, ? splits/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "987f669769974f2999f28993a027195f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating test examples...: 0 examples [00:00, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d3d9818133a64d19b01c7a68396d283d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Shuffling /root/tensorflow_datasets/imagenet_v2/topimages/incomplete.B6B260_3.0.0/imagenet_v2-test.tfrecord*..…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "720932a375d94345b0294e7ebcad83cb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset imagenet_v2 downloaded and prepared to /root/tensorflow_datasets/imagenet_v2/topimages/3.0.0. Subsequent calls will reuse this data.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2 Load the 'test' split (10000 images, top‑images variant)\n",
        "tfds_ds = builder.as_dataset(split=\"test\", as_supervised=True)"
      ],
      "metadata": {
        "id": "1YPI6jnCqKu5"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#3 Define a tiny IterableDataset wrapper\n",
        "class ImageNetV2TopImages(IterableDataset):\n",
        "    def __init__(self, tfds_dataset, transform=None):\n",
        "        self.ds = tfds_dataset\n",
        "        self.transform = transform\n",
        "\n",
        "    def __iter__(self):\n",
        "        for img, label in tfds.as_numpy(self.ds):\n",
        "            # img: HWC uint8 array, label: int\n",
        "            pil = Image.fromarray(img)\n",
        "            if self.transform:\n",
        "                pil = self.transform(pil)\n",
        "            yield pil, label"
      ],
      "metadata": {
        "id": "2vJVZ3nPrH5f"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#4 Torch transforms (same as ResNet expects)\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485,0.456,0.406],\n",
        "                         [0.229,0.224,0.225]),\n",
        "])"
      ],
      "metadata": {
        "id": "2zSfaVnnraYL"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#5 Instantiate DataLoader\n",
        "dataset = ImageNetV2TopImages(tfds_ds, transform=preprocess)\n",
        "loader  = DataLoader(dataset, batch_size=1, num_workers=4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UntGOOH3rdNt",
        "outputId": "9550afc1-cccf-4936-f583-593b894945af"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Define models and cascade logic\n",
        "import time\n",
        "import torch.nn.functional as F\n",
        "from torchvision import models\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load pretrained ResNets\n",
        "resnet18  = models.resnet18(pretrained=True).to(device).eval()    #model A\n",
        "resnet34  = models.resnet34(pretrained=True).to(device).eval()    #model B\n",
        "resnet152 = models.resnet152(pretrained=True).to(device).eval()   #model C\n",
        "\n",
        "# Thresholds\n",
        "SKIP_THRESH = 0.3   # if ResNet18 conf < 0.3, skip ResNet34\n",
        "THR_A = 0.7 # IDK threshold for model A\n",
        "THR_B = 0.7 # IDK thresholds for model B\n",
        "\n",
        "def predict(model, x):\n",
        "    logits = model(x)\n",
        "    probs  = F.softmax(logits, dim=1)\n",
        "    conf, cls = torch.max(probs, dim=1)\n",
        "    return cls.item(), conf.item()\n",
        "\n",
        "def get_prediction(model, x, thr=None):\n",
        "    cls, conf = predict(model, x)\n",
        "    if thr is not None and conf < thr:\n",
        "        return \"IDK\", conf\n",
        "    return cls, conf\n",
        "\n",
        "def dynamic_idk_cascade(x):\n",
        "    # A: ResNet18\n",
        "    cls_a, conf_a = get_prediction(resnet18, x, THR_A)\n",
        "    if cls_a != \"IDK\":\n",
        "        return cls_a, conf_a, \"ResNet18\"\n",
        "    # B: ResNet34\n",
        "    cls_b, conf_b = get_prediction(resnet34, x, THR_B)\n",
        "    if cls_b != \"IDK\":\n",
        "        return cls_b, conf_b, \"ResNet34\"\n",
        "    # C: ResNet152\n",
        "    cls_c, conf_c = predict(resnet152, x)\n",
        "    return cls_c, conf_c, \"ResNet152\""
      ],
      "metadata": {
        "id": "5EcBCL8JsT9G",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8050fe9d-e458-4991-cd9c-6fc0e2009bf8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 180MB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet34-b627a593.pth\" to /root/.cache/torch/hub/checkpoints/resnet34-b627a593.pth\n",
            "100%|██████████| 83.3M/83.3M [00:00<00:00, 156MB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet152_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet152_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet152-394f9c45.pth\" to /root/.cache/torch/hub/checkpoints/resnet152-394f9c45.pth\n",
            "100%|██████████| 230M/230M [00:01<00:00, 156MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import time\n",
        "\n",
        "# Prepare counters\n",
        "branch_sum = Counter()\n",
        "branch_cnt = Counter()\n",
        "\n",
        "# RUN & EVALUATE\n",
        "MAX_IMAGES = 10000\n",
        "total      = 0\n",
        "correct    = 0\n",
        "sum_time   = 0.0\n",
        "\n",
        "for imgs, labels in loader:\n",
        "    total += 1\n",
        "    imgs, labels = imgs.to(device), labels.to(device)\n",
        "\n",
        "    start = time.time()\n",
        "    pred, conf, branch = dynamic_idk_cascade(imgs)\n",
        "    elapsed = time.time() - start\n",
        "\n",
        "    # Update overall stats\n",
        "    correct += (pred == labels.item())\n",
        "    sum_time += elapsed\n",
        "\n",
        "    # Update branch‑specific stats\n",
        "    branch_sum[branch] += elapsed\n",
        "    branch_cnt[branch] += 1\n",
        "\n",
        "    # Light logging\n",
        "    if total % 500 == 0:\n",
        "        print(f\"[{total:5d}] Pred={pred:4d}  Used={branch:<20s}  Time={elapsed*1000:.1f}ms\")\n",
        "\n",
        "    if total >= MAX_IMAGES:\n",
        "        break\n",
        "\n",
        "# Core metrics\n",
        "accuracy  = correct / total\n",
        "avg_time  = sum_time / total          # seconds per image\n",
        "\n",
        "# Skip rate (A→C)\n",
        "skip_rate = branch_cnt.get(\"ResNet152 (skipped B)\", 0) / total\n",
        "\n",
        "# Print summary\n",
        "print(\"\\n=== SUMMARY ===\")\n",
        "print(f\"Total images         : {total}\")\n",
        "print(f\"Accuracy             : {accuracy*100:.2f}%\")\n",
        "print(f\"Avg. time per input  : {avg_time*1000:.1f} ms\")\n",
        "print(f\"Skip rate (A→C)      : {skip_rate*100:.2f}%\\n\")\n",
        "\n",
        "# Breakdown by branch\n",
        "for branch, cnt in branch_cnt.items():\n",
        "    t = branch_sum[branch]\n",
        "    print(f\"  {branch:>20s} | Count: {cnt:5d} | Avg time: {t/cnt*1000:.1f} ms\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "fZ_WQ3qshmk1",
        "outputId": "eded855a-5822-46c1-ac58-1df5dc07ae69"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[  500] Pred= 230  Used=ResNet152             Time=1011.4ms\n",
            "[ 1000] Pred= 225  Used=ResNet34              Time=249.0ms\n",
            "[ 1500] Pred=  35  Used=ResNet18              Time=91.9ms\n",
            "[ 2000] Pred= 244  Used=ResNet18              Time=92.0ms\n",
            "[ 2500] Pred= 786  Used=ResNet18              Time=93.4ms\n",
            "[ 3000] Pred= 420  Used=ResNet18              Time=90.8ms\n",
            "[ 3500] Pred= 895  Used=ResNet18              Time=92.2ms\n",
            "[ 4000] Pred= 999  Used=ResNet34              Time=253.5ms\n",
            "[ 4500] Pred=  17  Used=ResNet18              Time=98.5ms\n",
            "[ 5000] Pred= 200  Used=ResNet152             Time=1171.1ms\n",
            "[ 5500] Pred= 981  Used=ResNet34              Time=253.5ms\n",
            "[ 6000] Pred= 457  Used=ResNet18              Time=88.7ms\n",
            "[ 6500] Pred= 939  Used=ResNet152             Time=717.3ms\n",
            "[ 7000] Pred= 710  Used=ResNet18              Time=131.6ms\n",
            "[ 7500] Pred= 977  Used=ResNet152             Time=741.3ms\n",
            "[ 8000] Pred= 914  Used=ResNet152             Time=741.5ms\n",
            "[ 8500] Pred= 760  Used=ResNet18              Time=105.9ms\n",
            "[ 9000] Pred= 685  Used=ResNet18              Time=95.6ms\n",
            "[ 9500] Pred= 930  Used=ResNet152             Time=752.4ms\n",
            "[10000] Pred= 900  Used=ResNet152             Time=730.4ms\n",
            "\n",
            "=== SUMMARY ===\n",
            "Total images         : 10000\n",
            "Accuracy             : 79.04%\n",
            "Avg. time per input  : 309.2 ms\n",
            "Skip rate (A→C)      : 0.00%\n",
            "\n",
            "             ResNet152 | Count:  2596 | Avg time: 805.0 ms\n",
            "              ResNet18 | Count:  6044 | Avg time: 102.6 ms\n",
            "              ResNet34 | Count:  1360 | Avg time: 280.8 ms\n"
          ]
        }
      ]
    }
  ]
}
